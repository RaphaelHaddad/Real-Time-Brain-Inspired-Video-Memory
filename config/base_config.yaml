video:
  chunk_size_seconds: 5.0
  frames_per_chunk: 5
  fps_target: null

vlm:
  endpoint: "http://localhost:8078/v1"
  api_key: ""
  model_name: "qwen-vlm"
  temperature: 0.2
  top_p: 0.9
  max_tokens: 2048
  system_prompt: "You are a helpful assistant that describes video content in detail."
  user_prompt_template: "Describe what's happening in these frames from a video."

llm_injector:
  endpoint: "http://localhost:8078/v1"
  api_key: ""
  model_name: "qwen-vlm"
  temperature: 0.05
  top_p: 0.9
  max_tokens: 8192
  subgraph_extraction_injection: true

chunking:
  enabled: true
  chunk_size: 296
  chunk_overlap: 16
  max_triplets_per_chunk: 6
  use_sentence_boundaries: true
  parallel_count: 2
  enable_global_refinement: true
  refinement_max_tokens: 2000
  global_triplet_limit: 15  # Reduced from 25 to reduce token usage
  # Limits for instruction-based global refinement outputs
  max_new_triplets: 10  # Reduced from 20
  max_inter_chunk_relations: 5  # Reduced from 10
  max_merge_instructions: 4  # Reduced from 8
  max_prune_instructions: 4  # Reduced from 8
  # Per-chunk LLM timeout/retry
  chunk_timeout_seconds: 5.0
  chunk_timeout_retries: 3
  batch_llm_parallelism: false

kg:
  batch_size: 3
  verbose: true
  embedding_endpoint: "http://localhost:8071/v1"
  embedding_model: "qwen-embedding"
  embedding_api_key: ""

embedder:
  endpoint: "http://localhost:8071/v1"
  api_key: ""
  model: "qwen-embedding"
  top_k_chunk_with_batch_similarity: 3
  top_k_similar_batch: 2

neo4j:
  uri: "bolt://localhost:7687"
  user: "neo4j"
  password: "password"
  database: "neo4j"

retrieval:
  use_reranker: true
  reranker_endpoint: "http://localhost:8070/v1/rerank"
  reranker_api_key: ""
  reranker_model: "qwen-reranker"
  top_k: 5
  top_k_chunks: 8
  top_k_entities: 5
  top_k_relationships: 5
  graph_hops: 5
  post_compression: true
  compression_threshold: 0.15
  verbose: true
  entity_first: true
  rerank_after_traversal: true
  rerank_entities: true
  rerank_relationships: true

benchmark_llm:
  endpoint: "http://localhost:8078/v1"
  api_key: ""
  model_name: "qwen-vlm"
  temperature: 0.2
  top_p: 0.9
  max_tokens: 2048
# Save per-batch network metrics to a file while the run progresses
saving_batch_metrics: true