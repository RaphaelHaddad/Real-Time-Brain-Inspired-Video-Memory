video:
  chunk_size_seconds: 5.0
  frames_per_chunk: 5
  fps_target: null

vlm:
  endpoint: "http://localhost:8078/v1"
  api_key: ""
  model_name: "qwen-vlm"
  temperature: 0.2
  top_p: 0.9
  max_tokens: 2048
  system_prompt: "You are a helpful assistant that describes video content in detail."
  user_prompt_template: "Describe what's happening in these frames from a video."

llm_injector:
  endpoint: "http://localhost:8078/v1"
  api_key: ""
  model_name: "qwen-vlm"
  temperature: 0.05
  top_p: 0.9
  max_tokens: 8192
  subgraph_extraction_injection: true

chunking:
  enabled: true
  chunk_size: 296
  chunk_overlap: 16
  max_triplets_per_chunk: 6
  use_sentence_boundaries: true
  parallel_count: 2
  enable_global_refinement: true
  refinement_max_tokens: 4000
  global_triplet_limit: 15  # Reduced from 25 to reduce token usage
  # Limits for instruction-based global refinement outputs
  max_new_triplets: 10  # Reduced from 20
  max_inter_chunk_relations: 5  # Reduced from 10
  max_merge_instructions: 4  # Reduced from 8
  max_prune_instructions: 4  # Reduced from 8
  # Per-chunk LLM timeout/retry
  chunk_timeout_seconds: 5.0
  chunk_timeout_retries: 3
  batch_llm_parallelism: false
  max_connection_subgraph: 2

kg:
  batch_size: 3
  verbose: true
  embedding_endpoint: "http://localhost:8071/v1"
  embedding_model: "qwen-embedding"
  embedding_api_key: ""

embedder:
  endpoint: "http://localhost:8071/v1"
  api_key: ""
  model: "qwen-embedding"
  top_k_chunk_with_batch_similarity: 3
  top_k_similar_batch: 2

neo4j:
  uri: "bolt://localhost:7687"
  user: "neo4j"
  password: "password"
  database: "neo4j"

retrieval:
  use_reranker: true
  reranker_endpoint: "http://localhost:8070/v1/rerank"
  reranker_api_key: ""
  reranker_model: "qwen-reranker"
  top_k: 9
  top_k_chunks: 3
  top_k_entities: 5
  top_k_relationships: 5
  graph_hops: 2
  post_compression: true
  compression_threshold: 0.15
  verbose: false
  entity_first: true
  rerank_after_traversal: true
  rerank_entities: true
  rerank_relationships: true
  # Hop method: 'naive' (BFS traversal), 'page_rank' (PPR-guided), 'ch3_l3' (path-based with CH3-L3 scores)
  hop_method: ch3_l3
  # CH3-L3 specific retrieval parameters
  top_k_hop_ch3_l3: 4  # Top K candidates per hop for CH3-L3 traversal
  max_path_length_ch3: 4  # Maximum path length for CH3-L3 traversal
  cum_score_aggregation: product  # Score aggregation: 'product' or 'sum'
  # PageRank specific retrieval parameters
  top_k_hop_pagerank: 10  # Top K candidates per hop for PageRank traversal

benchmark_llm:
  endpoint: "http://localhost:8078/v1"
  api_key: ""
  model_name: "qwen-vlm"
  temperature: 0.2
  top_p: 0.9
  max_tokens: 2048


community_high_graph:
  community_creator: true  # Enable community-aware KG building with question generation
  community_retriever: true  # Enable community-based retrieval (similarity on CommunitySummary nodes)
  community_traversal_seed: chunk_node  # 'chunk_node' (default): seed traversal from community chunks; 'entity_node': seed from entities
  question_per_chunk: 2  # Number of questions to generate per chunk
  frequency_incremental_leiden: 5  # Run Leiden community detection every N batches
  community_resolution: 1.0  # Gamma parameter for Leiden algorithm resolution
  debug_print_community: 3  # Number of random community descriptions to log at DEBUG level

# Save per-batch network metrics to a file while the run progresses
saving_batch_metrics: true

# Pre-retrieval computation settings (PageRank, CH3-L3)
pre_retrieval_computation:
  auto_precompute_on_kg_build: false  # Run precomputation automatically after KG build
  pagerank:
    alpha: 0.15  # Teleport probability (damping = 1 - alpha)
    max_steps: 100  # Max power iteration steps (increased for convergence)
    top_k_per_node: 50  # Top K neighbors to store per node
    min_score: 0.001  # Minimum score threshold to store
  ch3_l3:
    candidates_per_node: 200  # Top K candidates to store per node
    min_ch3_score: 0.2  # Minimum CH3-L3 score threshold
    external_degree_approx: 2  # Approximation factor for external degree
    batch_size: 50  # Batch size for processing nodes